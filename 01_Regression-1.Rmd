---
title: "Chapter 1: Linear regression"
subtitle: "Simple Linear Regression"
author: "Joris Vankerschaver"
header-includes:
  - \useinnertheme[shadow=true]{rounded}
  - \usecolortheme{rose}
  - \setbeamertemplate{footline}[frame number]
  - \usepackage{color}
  - \usepackage{graphicx}
  - \graphicspath{{./images/01-linear-regression}}
output: 
  beamer_presentation:
    theme: "default"
    keep_tex: true
    includes:
      in_header: columns.tex
---


```{r include = FALSE}
CWD <- read.table("./datasets/01-linear-regression/christ.csv", header = T, sep = ",", dec = ".")
attach(CWD)

model <- lm(CWD.BASA ~ RIP.DENS)

```

# Introduction

## Regression

Statistical method with \alert{goal} to describe the relationship between 2 series of observations $(X_i,Y_i)$, obtained for individual subjects $i=1,...,n$

\begin{exampleblock}{Example} Basal area of coarse woody debris (CWD) versus tree density along 16 North American lakes
\begin{itemize}
\item \alert{Dependent variable, outcome, response} $Y$: CWD basal area 
\item \alert{Independent variable, explanatory variable, predictor} $X$: tree density (in number per km)
\end{itemize}
\end{exampleblock}

\centering
\includegraphics[width=0.30\textwidth]{riparian.png}

## Scatterplot with local regression smoother

\centering
```{r, echo=FALSE}
par(mfrow=c(1,1))
plot(RIP.DENS, CWD.BASA, xlab = "Riparian tree density", ylab = "CWD basal area")
lines(lowess(RIP.DENS, CWD.BASA,f=3/4), lty = 2)
abline(model)
```


## Regression

\begin{itemize}
\item For fixed $X$, $Y$ will not necessarily take on same value:
\begin{center}
\alert{observation = signal + noise}
\end{center}
\item Mathematical modelling of observation:
\[Y_i=f(X_i)+\epsilon_i\]
\item Define $f(x)$ as expected outcome for subjects with $X_i=x$
\[E(Y_i|X_i=x)=f(x)\] 
where $\epsilon_i$ is on average 0 for subjects with same $X_i$: \[E(\epsilon_i|X_i)=0\]
\end{itemize}

## Linear regression

\begin{itemize}
\item To obtain \alert{accurate} and \alert{interpretable} results, $f(X)$ is often chosen as linear function of unknown parameters
\item Use \alert{linear regression model}
\[E(Y|X=x)=\alpha + \beta x\]
with unknown \alert{intercept} $\alpha$ and \alert{slope} $\beta$
\item Linear regression model makes \alert{assumption} on distribution of $X$ and $Y$, so can be incorrect
\end{itemize}

## Use of linear regression

\begin{itemize}
\item \alert{Prediction}: when $Y$ unknown, but $X$ known, we can predict $Y$ based on $X$ 
\[E(Y|X=x)=\alpha + \beta x\]
\begin{itemize} 
\item \alert{Intercept}: $E(Y|X=0)=\alpha$
\end{itemize}
\vfill
\item \alert{Association}: describe biological relation between variable $X$ and continuous measurement $Y$
\begin{itemize} 
\item \alert{Slope}: 
\begin{eqnarray*}
E(Y|X=x+\delta)-E(Y|X=x)&=&\alpha + \beta (x+\delta) -\alpha-\beta x\\
&=& \beta\delta
\end{eqnarray*}
$\beta=$ difference in mean outcome between subjects that differ 1 unit in the value of $X$
\end{itemize}
\end{itemize}

## Least squares estimates

\begin{itemize}
\item \alert{Least squares (regression) line}: line that `best' fits data
\item Found by choosing values for $\alpha$ and $\beta$ that minimise sum of squares of \alert{residuals}: 
\[\sum_{i=1}^n (\underbrace{Y_i-\alpha-\beta X_i}_{\text{Residual}})^2\]
\item Estimates for $\beta$ and $\alpha$:
\[\hat{\beta}=\frac{\mbox{Cor}(x,y)S_y}{S_x} \quad \mbox{\rm and} \quad \hat{\alpha}=\bar Y - \hat{\beta} \bar X
\]
\end{itemize}

## 

```{r, echo=F}
a <- -77.10
b <- 0.12

x <- CWD$RIP.DENS
y <- CWD$CWD.BASA

plot_residuals <- function(a, b) {
  plot(y ~ x, xlab = "", ylab = "")
  abline(a, b)
  y_pred <- a + b*x
  segments(x, y_pred, x, y, lwd=2)
}

par(mfrow=c(1, 1))
plot_residuals(a, b)
text(1700, 100, paste("y =", a, "+", b, "x"))
```

## Output linear regression

```{r eval = FALSE}
model <- lm(CWD.BASA ~ RIP.DENS)
summary(model)
```
\begin{verbatim}
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -77.09908   30.60801  -2.519 0.024552 
RIP.DENS      0.11552    0.02343   4.930 0.000222 
\end{verbatim}
\[E(Y|X=x)=-77.10+0.12 x\]

## Output linear regression

\begin{itemize}
\item Model
\[E(Y|X=x)=-77.10+0.12 x\]
\item Expected CWD basal area is 1.2 m$^2$ larger alongside lakes with 10 more trees per km
\item Expected CWD basal area alongside lakes with 1,600 trees per km shoreline: 
\[-77.10 +0.12\times 1600=108 \ {\rm m}^2\]
\item Expected CWD basal area alongside lakes with 500 trees per km shoreline:
\[-77.10 +0.12\times 500=-17 \ {\rm m}^2\]
\item \alert{Be careful with extrapolation!} (linearity assumption can only be verified within range of data)
\end{itemize}

# Assumptions for linear models

## Verifying linearity assumption

\centering
```{r, echo=FALSE}
par(mfrow=c(1,1))
plot(RIP.DENS, CWD.BASA, xlab = "Riparian tree density", ylab = "CWD basal area")
lines(lowess(RIP.DENS, CWD.BASA,f=3/4), lty = 2)
abline(model)
```

## Verifying linearity assumption

\begin{itemize}
\item An alternative, that will be more convenient when there are multiple predictors, is a \alert{residual plot}
\begin{block}{Residuals} = prediction errors
\[e_i=y_i-\hat{\alpha}-\hat{\beta}x_i\]
\end{block}
\item If linear model correct, then scatterplot of $e_i$ versus $x_i$ or predictions $\hat{\alpha}+\hat{\beta}x_i$ shows no pattern
\end{itemize}

```{r eval = FALSE}
model <- lm(CWD.BASA ~ RIP.DENS)
par(mfrow=c(2,2))
plot(model)
```

## Verifying linearity assumption

\centering
```{r echo = FALSE}
model <- lm(CWD.BASA ~ RIP.DENS)
par(mfrow=c(2,2))
plot(model)
```

## Inference for simple linear regression

To be able to draw conclusions about the linear regression model
\[E(Y|X)=\alpha+\beta X\]
we need extra assumptions:
\begin{itemize}
\item \alert{Homoscedasticity}: for fixed $X$, $Y$ has constant variance
\[\mathrm{Var}(Y|X)=\sigma^2\]
that is estimated by the residual mean square error:
$$
\text{RMSE}=\sum_{i=1}^n e_i^2/(n-2)
$$
\item \alert{Normality}: for fixed $X$, $Y$ is normally distributed
\[Y|X \sim N(\alpha+\beta X, \sigma^2)\]
\end{itemize}

## Homoscedasticity versus heteroscedasticity

```{r, echo=FALSE}
set.seed(1234)
n <- 100
a <- -1
b <- 0.5

x <- seq(-3, 3, length.out = n)
y1 <- a + b*x + rnorm(n, sd = 0.5)

sd <- seq(1, 2, length.out = n)^3
y2 <- a + b*x + rnorm(n, sd = sd)

par(mfrow=c(1, 2))
plot(x, y1, xlab = "x", ylab = "y", main = "Homoscedasticity")
abline(a, b)
plot(x, y2, xlab = "x", ylab = "y", main = "Heteroscedasticity")
abline(a, b)
```

## Homoscedasticity?

\centering
```{r, echo=FALSE}
par(mfrow=c(1,1))
plot(RIP.DENS, CWD.BASA, xlab = "Riparian tree density", ylab = "CWD basal area")
lines(lowess(RIP.DENS, CWD.BASA,f=3/4), lty = 2)
abline(model)
```

## Assumption of homoscedasticity

\begin{itemize}
\item Squared residuals carry information on residual variability
\item If these are associated with explanatory variable, then indication of \alert{heteroscedasticity}
\item Scatterplot of $e_i^2$ or $\sqrt{|e_i|}$ versus $x_i$ or predictions
\end{itemize}

## Homoscedasticity?

\centering
```{r echo = FALSE}
plot(RIP.DENS, residuals(model)^2, xlab = "Riparian tree density", ylab = "Squared residuals")
lines(lowess(RIP.DENS, residuals(model)^2,f=3/4))
```

## Homoscedasticity?

\centering
```{r echo = FALSE}
model <- lm(CWD.BASA ~ RIP.DENS)
par(mfrow=c(2,2))
plot(model)
```


## Normality assumption

Assumption: outcomes normally distributed \alert{for fixed values of explanatory variable}
\[
  Y|X \sim N(a + bX, \sigma^2)
\]


Can be checked using QQ-plot of the residuals.

## Normality assumption valid

```{r, echo=FALSE}
set.seed(456)

n <- 200
a <- 0
b <- 2

x <- seq(1, 10, length.out = n)[1:(n-1)]
y <- a + b*x + rnorm(n - 1)
bin <- floor(x)

par(mfrow=c(1, 2))
plot(x, y, xlab = "", ylab = "")
boxplot(y ~ bin, xlab = "", ylab = "")
residuals <- y - a - b*x 

```

## Normality assumption not valid

```{r, echo=FALSE}
set.seed(456)

n <- 200
a <- 0
b <- 2

x <- seq(1, 10, length.out = n)[1:(n-1)]

e <- rnorm(n - 1)
scales <- seq(1, 2, length.out = n-1)^3
scales[e < 0] <- 1
e <- e*scales

y <- a + b*x + e
bin <- floor(x)

par(mfrow=c(1, 2))
plot(x, y, xlab = "", ylab = "")
boxplot(y ~ bin, xlab = "", ylab = "", side="right")
```

## QQ plot of residuals (Y|X normal)

```{r, echo=FALSE}
set.seed(456)

n <- 100
a <- 0
b <- 2

x <- seq(1, 10, length.out = n)[1:(n-1)]
y <- a + b*x + rnorm(n - 1)
bin <- floor(x)

par(mfrow=c(1, 2))
boxplot(y ~ bin, xlab = "", ylab = "", side="right")
residuals <- y - a - b*x 
qqnorm(residuals, main = "Q-Q plot of Y|X")
qqline(residuals)

```

## QQ plot of residuals (Y|X not normal)

```{r, echo=FALSE}
set.seed(456)

n <- 200
a <- 0
b <- 2

x <- seq(1, 10, length.out = n)[1:(n-1)]

e <- rnorm(n - 1)
scales <- seq(1, 2, length.out = n-1)^3
scales[e < 0] <- 1
e <- e*scales

y <- a + b*x + e
bin <- floor(x)

par(mfrow=c(1, 2))
boxplot(y ~ bin, xlab = "", ylab = "", side="right")
residuals <- y - a - b*x 
qqnorm(residuals, main = "Q-Q plot of Y|X")
qqline(residuals)
```

## Do not use QQ-plot of Y!

```{r, echo=FALSE}
set.seed(456)

n <- 100
a <- 0
b <- 2

x <- seq(1, 10, length.out = n)[1:(n-1)]
y <- a + b*x + rnorm(n - 1)
bin <- floor(x)

par(mfrow=c(1, 2))
residuals <- y - a - b*x 
qqnorm(residuals, main = "Q-Q plot of Y|X")
qqline(residuals)
qqnorm(y, main = "Q-Q plot of Y")
qqline(y)
```

## Normality?

\centering
```{r echo = FALSE}
model <- lm(CWD.BASA ~ RIP.DENS)
par(mfrow=c(2,2))
plot(model)
```

## What if homoscedasticity or normality false?

\begin{itemize}
\item \alert{Transformation of dependent variable} can help to obtain normality and homoscedasticity
\begin{exampleblock}{Example} $\sqrt{Y}, Y^2, 1/Y, \exp{Y}, \exp{-Y}, \ln{Y}$
\end{exampleblock}
\vfill
\item \alert{Transformation of independent variable} does not change distribution of $Y$ for given $X$:
\begin{itemize}
\item does not help in obtaining normality or homoscedasticity
\item does help to obtain linearity if normality and homoscedasticity ok
\end{itemize}
\end{itemize}

## What if homoscedasticity or normality false?

\begin{itemize}
\item Often heteroscedasticity or non-normality caused by fact that outcome can only take on values in certain interval and hence linear regression is not fit
\vfill
\item \alert{Solution}: transform outcome such that it can take on all real values
\begin{exampleblock}{Example} $\ln(Y)$ for positive outcomes $Y$
\end{exampleblock}
\vfill
\item \alert{Variance stabilising transformations}
\begin{exampleblock}{Example} $\arcsin{\sqrt{Y}}$ for proportions $Y$
\end{exampleblock}
\end{itemize}

## Transforming the outcome

\begin{itemize}
\item Heteroscedasticity possible because outcome positive, but linear model does not respect this 
\item $\ln$-transformation makes outcomes real-valued
\end{itemize}

\small
```{r eval=FALSE}
model2 <- lm(I(log(CWD.BASA)) ~ RIP.DENS)
summary(model2)
```

## Transforming the outcome

\footnotesize
```{r echo=FALSE}
model2 <- lm(log(CWD.BASA) ~ RIP.DENS)
summary(model2)
```

## Residual plots

```{r echo=FALSE}
par(mfrow=c(2,2))
plot(model2)
```

# Higher-order regression

## What if linearity assumption is false?

\begin{itemize}
\item Transformation of dependent variable 
\item Transformation of independent variable
\begin{exampleblock}{Example} If either independent or dependent variable right skewed, often log-transformation helps
\end{exampleblock}
\vfill
\item If residuals reveal \alert{quadratic association}, such that
\[e_i\approx \delta_0+\delta_1 x_i+\delta_2 x_i^2\]
then
\[y_i=\hat{\alpha}+\hat{\beta}x_i+e_i\approx
(\hat{\alpha}+\delta_0)+(\hat{\beta}+\delta_1)x_i+\delta_2
x_i^2\] 
\end{itemize}

## Quadratic regression

\begin{itemize}
\item We assume
\[E(Y|X)=\alpha+\beta X+\gamma X^2\]
\item Unknown parameters estimated by \alert{least squares method}: minimisation of
\[\sum_{i=1}^n (Y_i-\alpha-\beta X_i-\gamma X_i^2)^2\]
\end{itemize}
\vfill
\small
```{r eval=FALSE}
model3 <- lm(I(log(CWD.BASA)) ~ RIP.DENS + I(RIP.DENS^2))
summary(model3)
```

## Quadratic regression

\footnotesize
```{r echo=FALSE}
model3 <- lm(log(CWD.BASA) ~ RIP.DENS + I(RIP.DENS^2))
summary(model3)
```

## Residual plots

```{r echo=FALSE}
par(mfrow=c(2,2))
plot(model3)
```

## Building model proceeds hierarchically

\small
```{r echo=FALSE}
summary(model3)$coefficients
```

\normalsize
\begin{itemize}
\item Add terms to model and keep those as long as they are significantly associated with outcome
\begin{exampleblock}{Example} adding third order term is not significant contribution (p-value 0.26)
\end{exampleblock}
\item Adding proceeds \alert{hierarchically}: lower order terms are kept as long as higher order terms are in model
\end{itemize}

## Results

\begin{itemize}
\item We conclude 
\[E\{\ln(Y)|X\}=-9.69+0.017X-4.96 \ 10^{-6}X^2\]
or equivalently that geometric mean CWD basal area for given tree density $X$ is equal to
\[\exp(-9.69+0.017X-4.96 \ 10^{-6}X^2)\]
\vfill
\begin{exampleblock}{Example} for $X=500$ we now find 0.086 m$^2$ (previously: -17 m$^2$)
\end{exampleblock}
\vfill
\item How precise is this?
\end{itemize}

# Interpreting the results of a regression model

## Inference for mean outcome 

\begin{itemize}

\item $\hat{Y}_h=\hat{\alpha}+\hat{\beta}X_h$ is unbiased estimator of $E(Y|X_h)=\alpha+\beta X_h$
\item Standard error of $\hat{Y}_h$ is
\[SE(\hat{Y}_h)=\sqrt{MSE\left\{\frac{1}{n}+\frac{(X_h-\bar X)^2}{\sum_i (X_i-\bar X)^2}\right\}}\]
\item Highest precision for predictions in $X_h=\bar X$
\vfill
\item Tests and CI for $E(Y|X_h)$ based on
\[\frac{\hat{Y}_h-E(Y|X_h)}{SE(\hat{Y}_h)}\sim t_{n-p}\]
with $p$ number of unknown parameters in model
\end{itemize}

## Prediction in \texttt{R}

\small
```{r}
model3 <- lm(I(log(CWD.BASA)) ~ RIP.DENS + I(RIP.DENS^2))
p <- predict(model3, newdata = data.frame(RIP.DENS=800:2200), 
             interval = "confidence")
```
\begin{verbatim}
            fit          lwr        upr
             .            .           .
             .            .           .
             .            .           .
\end{verbatim}
```{r eval=FALSE}
exp(p$fit)
exp(p$lwr)
exp(p$upr)
```

## Expected outcome with 95\% CI

\centering
```{r echo=FALSE}
plot(RIP.DENS, CWD.BASA, xlab = "Riparian tree density", ylab = "CWD basal area", ylim = c(0, max(exp(p[,3]))))
abline(model, lty=3)
lines(800:2200, exp(p[,1]))
lines(800:2200, exp(p[,2]), lty=2)
lines(800:2200, exp(p[,3]), lty=2)
```


## Inference for $\beta$

\begin{itemize}
\item $\hat{\beta}$ unbiased estimator of $\beta$
\item Standard error of $\hat{\beta}$:
$$
SE(\hat{\beta})=\sqrt{\frac{MSE}{\sum_i (X_i-\bar X)^2}}
$$
with $MSE=\frac{1}{n}\sum_{i=1}^n(Y_i-\hat{Y}_i)^2$
\item Large spread on $X$ improves precision
\end{itemize}

## Spread and precision

\centering
\includegraphics[height = 7.5 cm , width = 10cm]{spread.pdf}

## Association tree density vs.\ CWD

Tests and confidence intervals for $\beta$ are based on
\[\frac{\hat{\beta}-\beta}{SE(\hat{\beta})}\sim t_{n-2}\]

\vfill

\small
```{r}
summary(model)$coefficients
```

## Association tree density vs.\ CWD
\alert{95\% CI} for $\beta$ needs $t_{14,0.975}=2.14$
\[[0.116 - 2.14\times 0.0234,0.116 + 2.14\times 0.0234]=[0.066,0.166]\]

\vfill

\small
```{r}
confint(model)
```
