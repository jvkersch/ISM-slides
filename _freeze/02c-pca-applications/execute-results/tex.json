{
  "hash": "479a7834c34d122fdb288da9ea295f97",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Principal component analysis: examples\"\nsubtitle: Introduction to Statistical Modelling\nauthor: Prof. Joris Vankerschaver\nformat:\n  beamer:\n    theme: Pittsburgh\n    colortheme: default\n    fonttheme: default\n    header-includes: |\n      \\setbeamertemplate{frametitle}[default][left]\n      \\setbeamertemplate{footline}[frame number]\n      \\hypersetup{colorlinks,urlcolor=blue}\n\n---\n\n\n\n\n## Examples\n\n1. Adulteration of olive oil\n  - Malavi, Derick, Amin Nikkhah, Katleen Raes, and Sam Van Haute. 2023. \"Hyperspectral Imaging and Chemometrics for Authentication of Extra Virgin Olive Oil: A Comparative Approach with FTIR, UV-VIS, Raman, and GC-MS.‚Äù Foods 12 (3): 429. \\url{https://doi.org/10.3390/foods12030429}\n2. Human faces dataset\n  - \\url{https://scikit-learn.org/0.19/datasets/olivetti_faces.html}\n  \n# Adulteration of olive oil\n\n## Problem setting\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\nExtra virgin olive oil (EVOO):\n\\vspace*{0.5cm}\n\n- High quality\n- Flavorful\n- Health benefits\n- **More expensive** (than regular oil)\n\n\\vspace*{1cm}\nTo reduce cost, EVOO is often **adulterated** with other, cheaper food oils.\n\n:::\n\n::: {.column width=\"50%\"}\n![](./images/02b-pca-applications/olive-oil.jpg){height=2in fig-align=center}\n:::\n::::\n\n## Research questions\n\n1. **Classification:** Can we detect whether a given EVOO sample has been adulterated?\n    - Yes/no answer (categorical)\n2. **Regression:** Can we detect the degree of adulteration?\n    - Continuous answer, from 0% (no adulteration) to 100%\n\n## Hyperspectral imaging (HSI)\n\n![](./images/02b-pca-applications/hyperspectral.png){fig-align=center height=50%}\n\n- Measures reflected infrared light (700-1800 nm) off sample\n- Provides a non-destructive way of testing sample\n\n\n## Hyperspectral \"images\" (spectra)\n\n![](./images/02b-pca-applications/hsi-spectra.png){fig-align=center height=50%}\n\n- HSI measures reflectance at 224 wavelengths from 700 to 1800 nm\n- Reflectance at given wavelength is determined by molecular features of sample\n\n## Experimental setup\n\nSamples to test (61 total):\n\n- 13 different kinds of unadulterated EVOO\n- 6 vegetable oils\n- 42 adulterated mixtures\n  - EVOO + one of 6 vegetable oils at one of 7 different percentages (from 1% to 20%)\n\nEach sample is imaged 3 times: **183 samples**\n\nEach sample produces a HSI spectrum of **length 224**\n\n## Data matrix\n\nData matrix has 183 rows (samples) and 224 columns (spectra).\n\nIn addition, we have some metadata:\n\n  - Name of sample\n  - Degree of adulteration\n\n![](./images/02b-pca-applications/dataset.png)\n\n## A first look at the data\n\nAveraged spectra for each kind of oil (EVOO + 6 others)\n\n![](./images/02b-pca-applications/spectra.png){fig-align=center height=60%}\n\nPlot shows small differences between spectra: **promising sign** that we will be able to address the research questions.\n\n## Principal component analysis: scree plot\n\nNot all 224 wavelengths are equally informative. Much of our dataset is redundant.\n\n![](./images/02b-pca-applications/scree-plot.png){fig-align=center height=50%}\n\nThis is confirmed by the scree plot: \n\n- First 2 PCs explain **94% of variance** in the data\n- First 3 PCs: almost 100%\n\n## Principal component analysis: loadings vectors\n\nLoadings vectors are linear combinations of features, tell us how features contribute to variability in dataset.\n\n![](./images/02b-pca-applications/first-two-pcs.png){fig-align=center height=50%}\n\nFor our example:\n\n- Loadings vector 1: where do spectra differ the most?\n- Loadings vector 2: where is next source of variability located?\n\n## Principal component analysis: scores\n\n![](./images/02b-pca-applications/oils-score-plot.png){fig-align=center height=60%}\n\nCan we tell pure and adulterated samples apart?\n\n- **Yes**: clearly different on score plot.\n\nCan we predict the percentage of adulteration?\n\n- **No**: hard to distinguish from first 2 PCs alone.\n\n## Predicting the percentage of adulteration\n\nWe will need more than 2 PCs to correctly predict percentage of adulteration.\n\nTwo different approaches:\n\n- **Principal component regression**: \n  1. Compute PCs\n  2. Do a regression on PCs\n\n- **Partial least squares regression**: \n  1. Compute factors that are most variable and **most correlated with outcome**\n  2. Do a regression on resulting factors\n\nBoth models can be built using the `pls` package in R.\n\n## Dataset\n\nFor this example we will use only the 42 adulterated mixtures.\n\nEach mixture is imaged 3 times: $42 \\times 3 = 126$ samples\n\nPredictors: 224 wavelengths\n\nOutcome: percentage of adulteration (1%-20%)\n\n## Performing a fair assessment: train/test split\n\nEvaluating the model using the same data used to train it leads to an **optimistic** estimate of the model's performance.\n\nTo avoid this bias, randomly select and set aside some data for testing, and use the remaining data to develop the model.\n\n![](./images/02b-pca-applications/traintest.pdf){fig-align=center}\n\nAdulteration prediction:\n\n- Train dataset: 101 samples\n- Test dataset: 25 samples\n\nCan you spot an issue with this?\n\n## Performing a fair assessment: data leakage\n\n- Each of the 42 mixtures is imaged 3 times.\n- Presumably these replicates are very similar\n- If some replicates end up in the test dataset and some in the train dataset: model gains unfair advantage.\n\n![](./images/02b-pca-applications/traintest-unstratified.pdf){fig-align=center}\n\n## Avoiding data leakage: stratified train/test split\n\nMain idea: develop model with some of the mixtures, test performance on different mixtures:\n\n1. Randomly select 80% of **mixtures**\n2. Put all 3 replicates for those 80% in the training set\n3. Put the remainder in the test set.\n\n![](./images/02b-pca-applications/traintest-stratified.pdf){fig-align=center}\n\n## Building the PCR/PLS models\n\nPCR model:\n\n```default\npcr_model <- pcr(\n  `% Adulteration` ~ ., data = adulterated_train, \n  scale = FALSE, validation = \"CV\", ncomp = 10\n)\n```\nPLS model: replace `pcr` by `plsr`.\n\nArguments:\n\n- `scale = FALSE`: Don't scale spectra (same units)\n- `ncomp = 10`: Build model with up to 10 components\n- `validation = \"CV\"`: Assess performance of model with $i$ components using cross-validation\n\n## Performance of PCR/PLS models\n\n![](./images/02b-pca-applications/regression-performance.png){fig-align=center height=60%}\n\nBoth models do well on the test data.\n\n## Optimal number of components: PCR\n\n(obtained via `selectNcomp(method = \"onesigma\")`)\n\n![](./images/02b-pca-applications/ncomp-pcr.png){fig-align=center height=60%}\n\n- Optimal number of components: 7\n- RMSEP for 7 components: 1.796\n\n## Optimal number of components: PLS\n\n\n![](./images/02b-pca-applications/ncomp-pls.png){fig-align=center height=60%}\n\n- Optimal number of components: 9\n- RMSEP for 9 components: 1.627\n\n## Conclusions\n\n*Can we detect whether a given EVOO sample has been adulterated?*\n\n  - **Yes**: Look at score plot\n  - More conclusive answer next lecture\n\n*Can we detect the degree of adulteration?*\n\n  - **Yes**: Build PCR or PLS model\n\n# Human faces dataset\n\n## \n\nThere are no slides for this part of the lecture. Instead, the lecture will follow the discussion in the following book chapter: https://jvkersch.github.io/ISM/pca-applications.html#sec-eigenfaces",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}