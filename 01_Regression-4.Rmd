---
title: "Chapter 1: Linear regression"
subtitle: "Outliers"
author: "Joris Vankerschaver"
header-includes:
  - \usepackage{multicol}
  - \usepackage{color}
  - \usepackage{Ghent}
  - \usepackage{alltt}
output: 
  beamer_presentation:
    theme: "AnnArbor"
    colortheme: "dolphin"
    keep_tex: true
---

```{r include = FALSE}
CWD <- read.table("./datasets/01-linear-regression/christ.csv", header = T, sep = ",", dec = ".")
attach(CWD)

model <- lm(CWD.BASA ~ RIP.DENS)
model3 <- lm(I(log(CWD.BASA)) ~ RIP.DENS + I(RIP.DENS^2))
summary(model)
confint(model)

needles <- read.table("./datasets/01-linear-regression/needles.txt", header = T, sep = "\t", dec = ".")
attach(needles)
model_l5 <- lm(length ~ nitrogen * phosphor + potassium 
               + phosphor * residu)

library(car)

body.fat <- read.table("./datasets/01-linear-regression/bodyfatNKNW.txt", header = T, dec = ".")
attach(body.fat)

birnh <- read.table("./datasets/01-linear-regression/birnh.txt", header = T, sep = "\t", dec = ".")
attach(birnh)
```

## Outliers / Influential observations 

\begin{itemize}
\item Dataset often contrains extreme values for outcome $Y$ and/or predictors $X$
\item These can influence regression line strongly
\end{itemize}

## Influence of influential observations 

\begin{center}
\includegraphics[height = 7.5 cm , width = 10cm]{images/temp/infl1.pdf}
\end{center}

## Influence of influential observations

\begin{center}
\includegraphics[height = 7.5 cm , width = 10cm]{images/temp/infl2.pdf}
\end{center}

## Influence of influential observations

\begin{center}
\includegraphics[height = 7.5 cm , width = 10cm]{images/temp/infl3.pdf}
\end{center}

## Influence of influential observations

\begin{center}
\includegraphics[height = 7.5 cm , width = 10cm]{images/temp/infl4.pdf}
\end{center}

## Influence of influential observations

\begin{center}
\includegraphics[height = 7.5 cm , width = 10cm]{images/temp/infl5.pdf}
\end{center}

## Influence of influential observations

\begin{center}
\includegraphics[height = 7.5 cm , width = 10cm]{images/temp/infl6.pdf}
\end{center}

## Tracking influential observations

\begin{itemize}
\item \alert{Residuals}: indicate how far outcome deviates from regression line
\item Hence, can be used to identify extreme outcomes
\end{itemize}

## Exteme outcomes in analysis larches?

\centering
```{r echo=FALSE}
qqnorm(model_l5$resid)
```

## Tracking influential observations

\begin{itemize}
\item \alert{Scatterplots} of outcome in function of predictors can be used to identify extreme outcomes and predictors
\item When multiple predictors, these plots have serious shortcomings
\end{itemize}


## Multivariate outliers: $Y$ versus $X_1$ or $X_2$

\begin{center}
\includegraphics[height = 7.5 cm , width = 10cm]{images/temp/infl7.pdf}
\end{center}

## Multivariate outliers: $X_1$ versus $X_2$

\begin{center}
\includegraphics[height = 7.5 cm , width = 10cm]{images/temp/infl8.pdf}
\end{center}

<!-- this observation has large leverage and Cook's distance -->

## Leverage

\begin{block}{Leverage (influence)} 
\begin{itemize}
\item Diagnostic measure to identify influential predictor-observations
\item Leverage of $i^{th}$ observation is: 
\begin{itemize}
\item measure for distance of predictor for $i^{th}$ observation to average predictor value
\item $i^{th}$ diagonal element of \alert{hat matrix} $H$: function of predictors $X$ that maps outcome on predictions
\[\textrm{prediction}=HY\]
\end{itemize}
\end{itemize}
\end{block}
The larger the leverage for $i^{th}$ observation, the closer $i^{th}$ prediction is to $i^{th}$ outcome

## Interpretation of leverage

\begin{itemize}
\item If leverage for $i^{th}$ observation large, then
\begin{itemize}
\item it has predictor values that deviate strongly from the mean
\item it \alert{possibly} has large influence on regression coefficients and predictions
\end{itemize}
\item Leverage is on average $p/n$ with $p$ number of unknown parameters
\item \alert{Extreme leverage}: larger than $2p/n$
\end{itemize}

## Leverage in analysis of larches

```{r echo=FALSE}
p=7
n=26
lev5 <- hatvalues(model_l5)
cutoff <- 2*p/n
plot(lev5, pch = 20, xlab = "", ylab = "Leverage")
abline(h=cutoff)
text(1.3, 0.91, 1)
text(4.3, 0.59, 4)
```

## Cook's distance

\begin{block}{Cook's distance} 
\begin{itemize}
\item Diagnostic measure for influence of $i^{th}$ observation on all predictions
\item Equivalent, for influence of $i^{th}$ observation on all estimated coefficients
\item Cook's distance for $i^{th}$ observation is obtained by comparing each prediction $\hat{Y}_j$ with prediction $\hat{Y}_{j(i)}$ that would be obtained \alert{if $i^{th}$ observation was deleted}
\[D_i=\frac{\sum_{j=1}^n(\hat{Y}_j-\hat{Y}_{j(i)})^2}{p\textrm{MSE}}\] 
\end{itemize}
\end{block}

## Interpretation Cook's distance

\begin{itemize}
\item If Cook's distance $D_i$ large, then $i^{th}$ observation has large influence on predictions and coefficients
\item \alert{Extreme Cook's distance}: exceeds  50\% percentile of $F_{p,n-p}$-distribution
\begin{exampleblock}{Example} 
\begin{itemize}
\item In analysis of larches is $p=7, n=26$ and the 50\% percentile of $F_{p,n-p}$-distribution 0.94
\item Cook's distance of first observation is 1.5 and corresponds to 77\% percentile
\item Conclusion: first observation has large influence on estimated regression coefficients 
\end{itemize}
\end{exampleblock}
\end{itemize}

## Cook's distance in analysis of larches

```{r echo=FALSE}
cd5 <- cooks.distance(model_l5)
plot(cd5, type = "h", ylab = "Cook's distance", xlab = "")
text(1.2, 1.5, 1)
text(6.2, 0.22, 6)
text(7.2, 0.24, 7)
```

## Analysis of larches: residual plots

\centering
```{r echo = FALSE}
par(mfrow=c(2,2))
plot(model_l5)
```

## DFBETAS

On what coefficient(s) will first observation have large influence?

\begin{block}{DFBETAS} 
\begin{itemize}
\item Diagnostic measure for influence of $i^{th}$ observation \alert{on each regression coefficient separately}
\item DFBETAS for $i^{th}$ observation and $j^{th}$ coefficient is obtained by comparing $j^{th}$ coefficient $\hat{\beta}_j$ with coefficient $\hat{\beta}_{j(i)}$ from model \alert{if $i^{th}$ observation would have been deleted}
\[\textrm{DFBETAS}_{j(i)}=\frac{\hat{\beta}_{j}-\hat{\beta}_{j(i)}}{\textrm{SD}(\hat{\beta}_{j})}\] 
\end{itemize}
\end{block}

## Interpretation DFBETAS

\begin{itemize}
\item Sign indicates if deleting observation $i$ causes an increase (DFBETAS$<0$) or decrease (DFBETAS$>0$) in each coefficient
\item \alert{Extreme DFBETAS}: exceeds 1 in small to moderate datasets, and $2/\sqrt{n}$ in large datasets
\end{itemize}

## DFBETAS in analysis of larches

```{r echo = FALSE}
dfb5 <- dfbetas(model_l5)[1,]
plot(dfb5, pch = 20, ylab = "DFBETAS", xlab = "Parameter number")
```

## DFBETAS in analysis of larches

First observation has \alert{large influence on interaction between phosphorus and residual ash}:
\begin{itemize}
\item current coefficient is -598.08 (SE 290.02);
\item DFBETAS is 2.17;
\item after deletion of first observation, interaction between phosphorus and residual ash will be around
\[-598.08-2.17\times 290.02=-1227.42\]
\end{itemize}

## Histogram and QQ-plot of interaction

```{r echo=FALSE}
inter5 <- phosphor * residu
par(mfrow=c(1,2))
hist(inter5, xlab = "phosphor * residual ash", main = "Histogram of phosphor * residu")
qqnorm(inter5)
```

## Analysis of larches after deletion $1^{st}$ observation

\small
```{r}
nitrogen2 <- nitrogen[-1]
phosphor2 <- phosphor[-1]
potassium2 <- potassium[-1]
residu2 <- residu[-1]
length2 <- length[-1]
model_l6 <- lm(length2 ~ nitrogen2 *  phosphor2 + potassium2 
               + phosphor2 * residu2)
```
```{r echo=FALSE}
summary(model_l6)$coefficients
```

## Analysis of larches after deletion interaction

\small
```{r}
model_l7 <- lm(length ~ nitrogen *  phosphor + potassium 
               + residu)
```
```{r echo=FALSE}
summary(model_l7)$coefficients
```

## Analysis of larches: final model

\small
```{r}
model_l8 <- lm(length ~ nitrogen *  phosphor + potassium)
```
```{r echo=FALSE}
summary(model_l8)$coefficients
```

## Final analysis: leverage

\centering
```{r echo=FALSE}
p2=5
lev8 <- hatvalues(model_l8)
cutoff2 <- 2*p2/n
par(mfrow=c(1,2))
plot(lev5, pch = 20, xlab = "", ylab = "Leverage")
abline(h=cutoff)
text(1.5, 0.91, 1)
text(4.5, 0.59, 4)
plot(lev8, pch = 20, xlab = "", ylab = "Leverage", ylim = c(0.0,0.9))
abline(h=cutoff2)
text(4.5, 0.59, 4)
```

## Final analysis: Cook's distance

\centering
```{r echo=FALSE}
cd8 <- cooks.distance(model_l8)
plot(cd8, type = "h", ylab = "Cook's distance", xlab = "")
text(4.2, 0.12, 4)
text(6.2, 0.17, 6)
text(7.2, 0.37, 7)
```

## Final analysis: residual plots

\centering
```{r echo=FALSE}
par(mfrow=c(2,2))
plot(model_l8)
```