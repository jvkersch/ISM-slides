---
title: "Nonlinear Modeling: Parameter Estimation"
subtitle: Introduction to Statistical Modelling
author: Prof. Joris Vankerschaver
pdf-engine: lualatex
format:
  beamer:
    theme: Pittsburgh
    colortheme: default
    fonttheme: default
    header-includes: |
      \setbeamertemplate{frametitle}[default][left]
      \setbeamertemplate{footline}[frame number]
      \usepackage{emoji}
---

```{r, include=FALSE}
library(tidyverse)

theme_set(theme_bw() + theme(text = element_text(size = 14)))
```


# Overview

## What is parameter estimation?

Determining the **optimal values for the parameters** using the experimental data, assuming that the structure of the model, or in other words, the relations between the variables and the parameters, are known.

## Objective function

## Minimizing the objective function

Note that this is done numerically and often painful

# Before doing parameter estimation

## Preparation: train/test split

## Preparation: determining parameters to estimate

## Preparation: determining good initial values

## Preparation: determining boundaries for parameters

-   Penalty function
-   Transformation

## Preparation: identifiability

# Minimizing the objective function

## Worked out example

## Nonlinear parameter estimation

## Local minima

## Minimization algorithms: gradient based

## Minimization algorithms: gradient-free

## Method of steepest descent

::: columns
::: {.column width="50%"}
You want to go down the mountain into the valley as efficiently as possible.

\vspace*{0.5cm}

The fog prevents you from seeing more than a few meters in every direction.

\vspace*{0.5cm}

How do you proceed?

\vspace*{0.5cm}

\emoji{light-bulb} Walk in the direction of **steepest descent**
:::

::: {.column width="50%"}
![](images/03a-parameter-estimation/cdf-wanderer.jpeg)
:::
:::

## Method of steepest descent: level sets

*Why is this called "level sets"*

Direction of steepest descent: **Gradient** \begin{eqnarray*}
s_k & = & -\nabla J(\theta^k) \\
& = & -\left[ \begin{array}{c}
                \frac{\partial J(\theta)}{\partial \theta_1} |_{\theta^k} \\
                \frac{\partial J(\theta)}{\partial \theta_2} |_{\theta^k} \\
                \vdots \\
                \frac{\partial J(\theta)}{\partial \theta_n} |_{\theta^k}
            \end{array}\right].
\end{eqnarray*}

The gradient is perpendicular to the level set at $\theta^k$.

## Method of steepest descent: Algorithm

*Split algorithm and step size over 2 slides*

Algorithm:

-   Compute gradient $\nabla J(\theta^k)$ at current value $\theta^k$.
-   Follow negative gradient to update $\theta^k$: $$
    \theta^{k+1} = \theta^k - \alpha_k \nabla J(\theta^k),
    $$ with $\alpha_k$ the step size.
-   Repeat until convergence

Step size $\alpha_k$ can be

-   Fixed: $\alpha_k = \alpha$ for a small fixed $\alpha$ (e.g. $\alpha = 0.01$).
-   Adaptive: determine the best $\alpha_k$ at each step.

## Method of steepest descent: variable step size

```{r, echo=FALSE}
source("scripts/03a-parameter-estimation//quadratic-function.R", local = knitr::knit_global())
```

## Method of steepest descent: disadvantages

```{r, echo=FALSE, fig.height=4.5}
source("scripts/03a-parameter-estimation//01-rosenbrock.R", local = knitr::knit_global())
```

-   Convergence can be slow (e.g for minimum hidden inside narrow "valley")
-   Steepest descent path will zigzag towards minimum, making little progress at each iteration.

## Method of Newton: 1D case

Find a minimum of $J(x)$ by solving $J'(x) = 0$.

::: columns
::: {.column width="60%"}
-   For a starting point $x_k$, look for a search direction $s_k$ such that $J'(x_k + s_k) \approx 0$.

-   Taylor: $J'(x_k + s_k)$ is approximately $$
    J'(x_k + s_k) \approx J'(x_k) + s_k J''(x_k).
    $$

-   Search direction: $$
    s_k = -\frac{J'(x_k)}{J''(x_k)}
    $$
:::

::: {.column width="30%"}
![](images/03a-parameter-estimation/newton2.jpeg)
:::
:::

Uses information from **first** and **second** derivatives.

## Method of Newton: properties

*Split this over two slides: one with convergence and comparison with steepest descent, and one with geometric interpreation*

For a quadratic function $J(x) = Ax^2 + Bx + C$, Newton's method finds the minimum in **one step**.

Geometric interpretation:

-   Approximate $J(x)$ around $x_k$ by best-fitting parabola.
-   Jump to bottom of parabola to find $x_{k+1}$.
-   Repeat!

```{r, echo=FALSE, fig.height=3}
source("scripts/03a-parameter-estimation//newton-polynomial.R", local = knitr::knit_global())
```

## Method of Newton: higher dimensions

-   Search direction uses gradient and **Hessian** $$
    s_k = -\left[ H(\theta^k)\right]^{-1} \nabla J(\theta^k)
    $$ where $$
    H(\theta^k) = \nabla^2 J(\theta^k) =
            \left[ \begin{array}{cccc}
                  \frac{\partial^2 J(\theta)}{\partial \theta_1^2} |_{\theta^k} & \frac{\partial^2 J(\theta)}{\partial \theta_1 \partial \theta_2} |_{\theta^k} & \cdots & \frac{\partial^2 J(\theta)}{\partial \theta_1 \partial \theta_n} |_{\theta^k} \\
                  \frac{\partial^2 J(\theta)}{\partial \theta_2 \partial \theta_1} |_{\theta^k} & \frac{\partial^2 J(\theta)}{\partial \theta_2^2} |_{\theta^k} & \cdots & \frac{\partial^2 J(\theta)}{\partial \theta_2 \partial \theta_n} |_{\theta^k} \\
                  \vdots & \vdots & \ddots & \vdots \\
                  \frac{\partial^2 J(\theta)}{\partial \theta_n \partial \theta_1} |_{\theta^k} & \frac{\partial^2 J(\theta)}{\partial \theta_n \partial \theta_2} |_{\theta^k} & \cdots & \frac{\partial^2 J(\theta)}{\partial \theta_n^2} |_{\theta^k}
              \end{array}\right]
    $$

## Method of Newton: advantages and disadvantages

-   Less iterations needed
-   Choice direction more efficient: descent and curvature

Disadvantages:

-   More sensitive to local extrema
-   First **and** second order differentials
-   Step size $\alpha=1$. If initial vector too far from minimum, method will often not converge to minimum.

## Method of Newton: convergence

```{r, echo=FALSE, fig.height=4.5}
source("scripts/03a-parameter-estimation//rosenbrock-newton.R", local = knitr::knit_global())
```

-   Very fast convergence for Rosenbrock function (3 iterations)
-   In general: **quadratic convergence**

## Many advanced gradient-based methods exist

-   Broyden-Fletcher-Goldfarb-Shanno (BFGS): approximation of Hessian
-   Levenberg-Marquardt: very popular, combines
    -   Steepest descent: robust but slow
    -   Method of Newton: fast, but often not convergent
-   Powell/Brent: search along set of directions

::: {.callout-note}
## Optimization in R

Use `optim(par, fn)`, where

- `par`: initial guess
- `fn`: the function to optimize
- `method`: "Nelder-Mead" (default), "BFGS", "Brent", ...
:::


## Simplex algorithm (Nelder-Mead 1965)


Basic idea: Capture optimal value inside simplex (triangle, pyramid, ...)

- Start with random simplex.
- Adjust worst corner of simplex by using different "actions".
- Repeat until convergence.


![](images/03a-parameter-estimation/nelder-mead-actions.pdf){fig-align=center}


## Simplex algorithm

![](images/03a-parameter-estimation/nelder-mead.pdf)

## Simplex algorithm: advantages and disadvantages

- Does not require gradient, Hessian, ... information
- Robust: often finds a minimum where other optimizers cannot.

- Can find a rough approximation of a minimum in just a few updates ...
- ... but may take a long time to converge completely.

## Global minimisation

- Disadvantage local techniques: local minima can never be completely excluded

- Global techniques insensitive to this problem

- Disadvantage: needs a lot of evaluations of $J$

- Types:

    - Gridding

    - Random methods

## Global minimisation: Gridding

- Evaluate $J$ for a grid of parameter values $\theta$

- Select minimum among grid values

```{r, echo=FALSE, fig.height=6}

source("scripts/03a-parameter-estimation//gridding.R", local = knitr::knit_global())

```

## Global minimisation: Gridding

The finer the grid:

- the more likely to find the optimum,
- BUT the more calculations needed

Iterative:

- Start with a coarse-grained grid
- Refine parameter domain and repeat

Brute force, inefficient

## Global minimisation: Random methods

Evaluate $J$ for random parameter sets

- Choose PDF for each parameter
- Random sampling; Latin hypercube sampling

Retain

- Optimal set (with $J_{min}$)
- Some sets below certain critical value ($J_{crit}$)

Examples:

- Genetic algorithms
- Shuffled complex evolution
- Ant colony optimization
- Particle swarm optimization
- Simulated annealing
- ...


# Assessing the quality of a fit

## Residuals

Model:
$$
  y = f(x; \theta) + \epsilon
$$
where $\epsilon$ is normally distributed.

If the model is well-fit, the residuals $e_i = y_i - f(x_i; \theta)$ should not show any systematic patterns.

- Independence
- Mean 0
- Constant variance

# Correlations in time series (Optional)

## Residuals: correlation and independence

- We often assume that residuals are independent. But this is not always the case, especially in **time series**.
- Correlations in residuals are often a sign that something is missing from model fit.

How can we detect patterns, correlations, ... in residuals?

```{r, include=FALSE}
source("scripts/03a-parameter-estimation/autocorrelation.R", local = knitr::knit_global())
```

\vspace*{1cm}

```{r, echo=FALSE, fig.height=3}
library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
```


## Autocorrelation: how are residuals related?

**Autocorrelation** with lag $\tau$ answers the following questions:

- To what extent does a residual depend on a previous residual?
- Is there correlation between residuals in time?

$$
  r_\varepsilon(\tau) = \frac{1}{r_\varepsilon(0)}\sum_{k=1}^{N-\tau} \frac{\varepsilon(t_k) \cdot \varepsilon(t_k+\tau)}{N-\tau}
$$

where $r_\varepsilon(0)=\sum_{k=1}^{N} \frac{\varepsilon^2(t_k)}{N}$

## Detecting significant autocorrelations

If data is uncorrelated, then autocorrelation is normally distributed:
$$
  r_\varepsilon(\tau) \sim N\left(0, \frac{1}{\sqrt{N}}\right).
$$
Can be used to detect "abnormally high" correlations: 

- Only about 5% of values outside range $\pm 1.96/\sqrt{N}$.
- If more, sign that data is correlated.


## Example: Energy consumption in Korea (2017-19)

Autocorrelation uncovers repeating patterns in signal:

- Highly correlated over 12-month basis
- Anticorrelated over 6-month basis

```{r, include=FALSE}
library(gridExtra)
source("scripts/03a-parameter-estimation/energy.R", local = knitr::knit_global())
```

\vspace*{0.5cm}

```{r, echo=FALSE, warning=FALSE, fig.height=3}
library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
```

Source: Korea Energy Economics Institute.

## How to deal with correlations in residuals?

- **Make model bigger** (next slides)
- Subsample data to reduce strength of correlations: not recommended
- Use modelling technique that does not need uncorrelated residuals (e.g. autoregressive models)

## Example: Calcium flows (simulated data)

Over the course of exercise, calcium ions flow in and out of the muscle cells. On biological grounds, model calcium concentration as exponentially damped sine:
$$
  C(t) = \exp(-A t) \sin(t)
$$

Data and model fit:
```{r}
#| out-width: 3in
#| out-height: 2in
#| fig-width: 4.5
#| fig-height: 3
#| fig-align: center

set.seed(1234)

t <- seq(0, 20, length.out = 50)
y_large <- exp(-t/10)*sin(t)
y_perturb <- 0.2 * cos(t)
y_noise <- 0.1 * rnorm(length(t))

y_full <- y_large + y_perturb + y_noise

fit <- nls(y ~ exp(-t/A)*sin(t),
           data = data.frame(t = t, y = y_full),
           start = list(A = 10))
A_fitted <- coef(fit)["A"]

tibble(t = t, y_full = y_full, y_predict = predict(fit)) |>
  ggplot() +
  geom_point(aes(t, y_full)) +
  geom_line(aes(t, y_predict)) +
  xlab("Time") +
  ylab("Calcium conc.")

```

## Residual plot

Model fit is good, but not perfect. Clear **repeating pattern** in the residuals.

```{r}
#| out-width: 3in
#| out-height: 2in
#| fig-width: 4.5
#| fig-height: 3
#| fig-align: center
resids <- y_full - predict(fit)
tibble(t = t, resids = resids) |>
  ggplot(aes(t, resids)) +
  geom_point() +
  geom_line(linetype = "dashed") +
  xlab("Time") +
  ylab("Residual")
```

## Autocorrelation plot

Lack of model fit, repeating pattern in the residuals can also be seen from the autocorrelation plot.

```{r}
#| out-width: 3in
#| out-height: 2in
#| fig-width: 4.5
#| fig-height: 3
#| fig-align: center

autocorr_plot <- function(x, plot_thresholds = FALSE, conf_level = 0.95) {
  plotdata <- with(acf(x, plot = FALSE), 
                   data.frame(lag, acf))

  p <- ggplot(plotdata, aes(x = lag, y = acf)) +
         geom_bar(stat = "identity", position = "identity") +
    xlab("Lag") +
    ylab("ACF")

  if (plot_thresholds) {
    threshold <- qnorm((1 - conf_level) / 2) / sqrt(length(x))
    p <- p +
        geom_hline(yintercept=c(threshold, -threshold),
                   linetype="dashed", color = "red", linewidth = 1)
  }
  
  p
}

autocorr_plot(resids, plot_thresholds = TRUE)
```

- Red lines: thresholds $1.96 / \sqrt{50} = 0.227$.
- 13 out of 17 autocorrelations (76%) exceed threshold 


## Expanding the model

Pattern in residuals is a clear sign that **something is missing** in our modelling approach. Given the periodic oscillations, propose
$$
  C(t) = \exp(-At)\sin(t) + B\cos(\omega t).
$$


```{r}
#| out-height: 2in
#| out-width: 4in
#| fig-height: 3
#| fig-width: 6
#| fig-align: center

fit_full <- nls(y ~ exp(-t/A)*sin(t) + B*cos(C*t),
            data = data.frame(t = t, y = y_full),
            start = list(A = 10, B = 1.0, C = 1.0))
A_fitted <- coef(fit)["A"]
B_fitted <- coef(fit)["B"]
C_fitted <- coef(fit)["C"]

easy_predict <- function(model, t) {
  predict(model, newdata = data.frame(t = t))
}

data <- tibble(t = t, y_full = y_full)
t_dense <- seq(min(t), max(t), length.out = 500)
curves <- tibble(
  t = t_dense,
  y_p = easy_predict(fit, t_dense),
  y_p_full = easy_predict(fit_full, t_dense))

ggplot() +
  geom_point(data = data, aes(t, y_full)) +
  geom_line(data = curves,
            aes(t, y_p, linetype = "Original"),
            color = "gray50") +
  geom_line(data = curves, 
            aes(t, y_p_full, linetype = "Expanded")) +
  xlab("Time") +
  ylab("Calcium conc.") +
  scale_linetype_manual(
    values = c("Expanded" = "solid", "Original" = "dashed"),
    name = "Model")

```

## Residual and autocorrelation plot

No residual pattern visible in residuals. The model is well fit.

```{r}
#| out-width: 4.5in
#| out-height: 2in
#| fig-width: 7
#| fig-height: 3
#| fig-align: center

library(gridExtra)

resids_full <- y_full - predict(fit_full)
p <- tibble(t = t, resids = resids_full) |>
  ggplot(aes(t, resids)) +
  geom_point() +
  geom_line(linetype = "dashed") +
  xlab("Time") +
  ylab("Residual")

q <- autocorr_plot(resids_full, plot_thresholds = TRUE)

grid.arrange(p, q, ncol = 2)
```

## Residual QQ-plots

```{r}
#| out-width: 4.5in
#| out-height: 2in
#| fig-width: 7
#| fig-height: 3
#| fig-align: center

library(gridExtra)

easy_qqplot <- function(data) {
  ggplot(tibble(sample = data), aes(sample = sample)) +
    stat_qq_line(color = "gray") + stat_qq() + 
    xlab(NULL) +
    ylab(NULL)
}

p <- easy_qqplot(resids) +
  ggtitle("Original model")
q <- easy_qqplot(resids_full) +
  ggtitle("Expanded model")

grid.arrange(p, q, ncol = 2)

```