---
title: "Logistic Regression and Classification"
subtitle: Introduction to Statistical Modelling
author: Prof. Joris Vankerschaver
format:
  beamer:
    theme: Pittsburgh
    colortheme: default
    fonttheme: default
    header-includes: |
      \setbeamertemplate{frametitle}[default][left]
      \setbeamertemplate{footline}[frame number]

---

## Logistic regression

In many regression problems, the outcome is a **categorical** variable:

- Figure out whether mutation is deleterious (yes/no), based on DNA sequencing data.
- Predict a person's eye color (blue/brown/green)
- Cancer yes/no

## Datasets

1. `bdiag` -- Wisconsin breast cancer diagnostic dataset (*Nuclear feature extraction for breast tumor diagnosis.*
W. Street, W. Wolberg, O. Mangasarian. Electronic imaging 29 (1993))

  - Cell nuclei from 569 tumor samples
  - Classified as malignant or benign
  - Features:
    - radius of the cell nucleus
    - texture (variance of gray-scale values)

```{r}
#| include: false
library(tidyverse)
library(gridExtra)

theme_set(theme_bw() + theme(text = element_text(size = 14)))

bdiag <- read_csv("datasets/02-logistic-regression/bdiag.csv") |>
  mutate(diagnosis = as.factor(diagnosis),
         diagnosis_binary = ifelse(diagnosis == "B", 0, 1))

```


## Train/test split

**note to self: train/test split only necessary to evaluate predictive performance?**

For a fair assessment of our model, split data into:

- **Train** data: used to develop the model (typically 70-80%)
- **Test** data: used to evaluate the model (remaining 20-30%). This data is **not used during model development**.

Not using all data to develop the model has disadvantages (cf. cross-validation).

```{r}
#| echo: true
set.seed(1234)

train_size <- 0.80 * nrow(bdiag)
train_ind <- sample(seq_len(nrow(bdiag)),
                    size = train_size)

train <- bdiag[train_ind, ]
test <- bdiag[-train_ind, ]
```

## A first look at the data

```{r}
#| echo: false

p_scatter <- ggplot(train, 
                    aes(x = radius_mean, y = texture_mean, color = diagnosis)) +
  geom_jitter() +
  theme(legend.position = "top")

p_box_radius <- ggplot(train, 
                       aes(y = radius_mean, x = diagnosis, fill = diagnosis)) +
  geom_boxplot(show.legend = FALSE) +
  ggtitle("radius_mean") + xlab("") + ylab(NULL)

p_box_texture <- ggplot(train, 
                        aes(y = texture_mean, x = diagnosis, fill = diagnosis)) +
  geom_boxplot(show.legend = FALSE) +
  ggtitle("texture_mean") + xlab("") + ylab(NULL)


grid.arrange(p_scatter, p_box_radius, p_box_texture, nrow = 1, widths = c(2, 1, 1))
```

## Reminder: odds and  odds-ratios

- If $p$ is the probability of having a malignant tumor, then the **odds** are 
$$
  \mathrm{Odds} = \frac{p}{1 - p}.
$$
For example: if $p = 0.9$ then $\mathrm{Odds} = 9$.

- Odds range from 0 (impossible event) to $+\infty$ (almost certain).

- **Odds ratio**: 

## Log-odds (logits)

Often it makes sense to work with the logarithm of the odds (**logits**):
$$
  \mathrm{logit} = \log \left( \frac{p}{1 - p} \right).
$$
To convert back to probabilities, use the sigmoid or **logistic** function:
$$
  p = \frac{1}{1 + e^{-\mathrm{logit}}}.
$$

Logits are unbounded: $\mathrm{logit} \to \pm\infty$ for $p \to 0, 1$

```{r}
#| fig-height: 3
ggplot(tibble(x = seq(-5, 5, length.out = 100)), aes(x)) +
  geom_function(fun = plogis) +
  xlab("Logit") +
  ylab("Probability")
```

## Regression for classification

- Notation:
  - $Y = 0, 1$: categorical outcome
  - $X_1, \ldots, X_p$: predictors (can be continuous or discrete)

- Idea: model $p(x) = P(Y = 1 | X = x)$


## Why does linear regression not work?

- One predictor $X = \mathtt{radius\_mean}$, outcome $Y = 0, 1$.
- Assume $p(x) = \alpha + \beta x$ and determine $\alpha, \beta$ through linear regression.

```{r}
#| out-width: 2in
#| out-height: 1.5in
#| fig-align: center
ggplot(train, aes(x = radius_mean, y = diagnosis_binary)) +
  geom_point(aes(color = diagnosis)) +
  stat_smooth(method="lm", se=FALSE, color = "gray40") +
  ylab("Probability")

```

Problems:

- Fitted probabilities can take on values outside $[0, 1]$.
- Does not easily generalize to more than two classes.

## Logistic regression

- The outcome $Y$ is a binomial variable with probability $p(X)$
- The probability $p(X)$ depends on $X$ through the logistic function
$$
  p(X) = \frac{1}{1 + \exp(-(\alpha + \beta X))}.
$$
- **Nonlinear** model in parameters $\alpha$, $\beta$


## Determining the regression parameters: MLE

- Likelihood function $\mathcal{L}$: probability of observing the data given the parameters $\alpha$, $\beta$:
$$
  \mathcal{L}(\alpha, \beta) = \prod_{i = 1}^n P(Y = y_i | X = x_i),
$$
where 
$$
  P(Y = y_i | X = x_i) = p(x_i)^{y_i}(1 - p(x_i))^{1- {y_i}}.
$$
is the probability of observing one data point $(x_i, y_i)$.

- In practice, the log of the likelihood is often used:
$$
  \ell(\alpha, \beta) = \ln \mathcal{L}(\alpha, \beta).
$$

## Determining the regression parameters: MLE

- **Maximum likelihood estimation** (MLE): find parameters that maximize $\mathcal{L}(\alpha, \beta)$ or $\ell(\alpha, \beta)$
- Finding maximum: set partial derivatives (score functions) equal to zero:
$$
  \frac{\partial \ell}{\partial \alpha} = 0, \quad
  \frac{\partial \ell}{\partial \beta} = 0.
$$
- Complicated equations, maximum cannot be found analytically (unlike least squares)
- Done in R using the `glm` function

## In R

\scriptsize
```{r}
#| echo: true
m_simple <- glm(diagnosis ~ radius_mean, data = train, family = "binomial")
summary(m_simple)
```

## The (log of the) likelihood function

```{r}
log_lh <- function(x, y, alpha, beta) {
  lp <- alpha + beta * x
  px <- 1 / (1 + exp(-lp))
  
  llh <- log(px)
  llh[y == 0] <- log(1 - px[y == 0])
  sum(llh)
}

alpha <- seq(-20, -10, length.out = 20)
beta <- seq(0, 2, length.out = 20)

x <- train$radius_mean
y <- train$diagnosis_binary

llh <- matrix(nrow = length(alpha), ncol = length(beta))
for (i in seq_along(alpha)) {
  for (j in seq_along(beta)) {
    llh[i, j] <- log_lh(x, y, alpha[[i]], beta[[j]])
  }
}

filled.contour(alpha, beta, llh,
               xlab = "alpha",
               ylab = "beta",
               plot.axes = {
                 axis(1)
                 axis(2)
                 points(-15.8086, 1.0662, pch = "x", cex = 2, col = "white")
               })

```


## Multiple logistic regression

- Like in linear regression, often the outcome $Y$ is influenced by several predictors $X_1, X_2, \ldots, X_p$.
- For example: `diagnosis` depends on `radius_mean` and `texture_mean`:
\begin{multline*}
  \mathrm{logit}(\mathtt{diagnosis}) = \\
  \alpha +
  \beta_1 \cdot \mathtt{radius\_mean} +
  \beta_2 \cdot \mathtt{texture\_mean}.
\end{multline*}
- Parameters $\alpha, \beta_1, \ldots, \beta_p$ determined through MLE.

## In R

\scriptsize
```{r}
#| echo: true
m_multi <- glm(diagnosis ~ radius_mean + texture_mean,
               data = train, family = "binomial")
summary(m_multi)
```




## Quantifying the strength of an association

Write the logistic regression model in terms of odds as
$$
  \log \mathrm{Odds}(x) = \alpha + \beta x.
$$
After some algebra:
$$
  e^\beta = \frac{\mathrm{Odds}(x + 1)}{\mathrm{Odds(x)}}.
$$
In other words: $e^\beta$ is the odds ratio (OR) associated to a 1-unit increase in $x$.

::: {.callout-note}
## Breast cancer dataset
Here $\beta = 1.0662$, so $OR = \exp(1.0662) = 2.90$. An increase in 1 mm in tumor radius is associated with odds 
that are 2.90 times higher.
:::

## Testing an association

- Is the model with $x$ "better"? XXX rephrase
- $H_0: \beta = 0$ and $H_A: \beta \ne 0$.

Several ways of testing:

- Wald test: reported by R
- Likelihood ratio test: preferred if you can do it
- Score test (not covered)

## Testing an association: Wald test

To test $H_0: \beta = 0$ against $H_A: \beta \ne 0$, compute the test statistic 
$$
  z = \frac{\hat{\beta}}{SE(\beta)} \sim N(0, 1)
  \quad \text{under $H_0$}.
$$

This is reported in the R regression output:
```{r}
summary(m_simple)$coefficients
```

::: {.callout-warning}
## Warning

Suffers from ...
:::


## Testing an association: Likelihood ratio test

Recall:

- $\ell(\alpha, \beta)$: likelihood function
- $\hat{\alpha}$, $\hat{\beta}$: maximum-likelihood estimates of parameters

**Likelihood ratio** test: compute the **deviance**
$$
  D = -2 \ln \frac{
    \ell(H_0)}{\ell(\hat{\alpha}, \hat{\beta})}
  \sim \chi^2_1 \quad \text{under $H_0$}
$$

... this would be better off after multiple regression ...

## Worked out example





## Confidence intervals for regression parameters

## Making predictions

What is the probability of a tumor being malignant if the radius is 13 mm?

- By hand: 
  $$
    p(x = 13) = \frac{1}{1 + \exp(15.8086 - 1.0662 \times 13)} = 0.1247716
  $$
- Using R:

```{r}
#| echo: true
predict(m_simple,
        newdata = data.frame(radius_mean = 13), 
        type = "response")

```

## Making predictions

```{r}
ggplot(train, aes(x = radius_mean, y = diagnosis_binary)) +
  geom_vline(xintercept = 13, linetype = "dashed", color = "gray60") +
  geom_hline(yintercept = 0.1247961, linetype = "dashed", color = "gray60") +
  geom_jitter(aes(color = diagnosis), height = 0.10) +
  stat_smooth(method="glm", se=FALSE, color = "gray40",
              method.args = list(family=binomial))
```





# Draft outline



## Linear discriminant analysis (LDA)

Why another classification technique?

- More stable when classes are well separated
- More stable for small $n$
- Easy to use when more than two response classes

Results often surprisingly similar. No method will outperform all the others all the time.

Other classifiers:
- $k$-nearest neighbors
- Neural networks
- Support vector machines (SVM)

## Reminder: Bayes

$$
  P(Y = k | X = x) = \frac{P(X = x | Y = k) P(Y = k)}{\sum P(X = x | Y = k) P(Y = k)}
$$

Need to model:
- Prior probabilities $\pi_k$
- Class functions $f_k(x)$

We get: posterior probabilities $p_k(X)$

## Gaussian assumption

$f_k(x)$ are Gaussians with **same variance**

Derive discriminant functions


## Estimating the LDA parameters

## Worked out example

Simulated data, well separated, linear decision boundary is line orthogonal to centers (if noise is isotropic)

## In R


## More than two predictors

## Comparison logistic and linear regression


## References

- 

## Note to self

- Find compelling case studies
- Make sure decision boundary is clearly introduced
- Figure out the wald test

## Case studies

- Synthetic data
- Iris, for multiple classes
- Zuur et al (modeling ecological data): S. Solea vs salinity. 61 observations
- VGSM: WCGS dataset (large, multiple predictors)
- Le et al: Prostate cancer dataset. Multiple predictors, small-ish
- Pupal color.


