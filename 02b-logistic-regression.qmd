---
title: "Logistic Regression and Classification"
subtitle: Introduction to Statistical Modelling
author: Prof. Joris Vankerschaver
format:
  beamer:
    theme: Pittsburgh
    colortheme: default
    fonttheme: default
    header-includes: |
      \setbeamertemplate{frametitle}[default][left]
      \setbeamertemplate{footline}[frame number]
---

## Logistic regression

In many regression problems, the outcome is a **categorical** variable:

- Figure out whether mutation is deleterious (yes/no), based on DNA sequencing data.
- Predict a person's eye color (blue/brown/green)
- Cancer yes/no

## Datasets

1. `bdiag` -- Wisconsin breast cancer diagnostic dataset (*Nuclear feature extraction for breast tumor diagnosis.*
W. Street, W. Wolberg, O. Mangasarian. Electronic imaging 29 (1993))

  - Cell nuclei from 569 tumor samples
  - Classified as malignant or benign
  - Features:
    - radius of the cell nucleus
    - texture (variance of gray-scale values)

```{r}
#| include: false
library(tidyverse)
library(gridExtra)

theme_set(theme_bw() + theme(text = element_text(size = 14)))

bdiag <- read_csv("datasets/02-logistic-regression/bdiag.csv") |>
  mutate(diagnosis = as.factor(diagnosis),
         diagnosis_binary = ifelse(diagnosis == "B", 0, 1))

```


## Train/test split

**note to self: train/test split only necessary to evaluate predictive performance?**

For a fair assessment of our model, split data into:

- **Train** data: used to develop the model (typically 70-80%)
- **Test** data: used to evaluate the model (remaining 20-30%). This data is **not used during model development**.

Not using all data to develop the model has disadvantages (cf. cross-validation).

```{r}
#| echo: true
set.seed(1234)

train_size <- 0.80 * nrow(bdiag)
train_ind <- sample(seq_len(nrow(bdiag)),
                    size = train_size)

train <- bdiag[train_ind, ]
test <- bdiag[-train_ind, ]
```

## A first look at the data

```{r}
#| echo: false

p_scatter <- ggplot(train, 
                    aes(x = radius_mean, y = texture_mean, color = diagnosis)) +
  geom_jitter() +
  theme(legend.position = "top")

p_box_radius <- ggplot(train, 
                       aes(y = radius_mean, x = diagnosis, fill = diagnosis)) +
  geom_boxplot(show.legend = FALSE) +
  ggtitle("radius_mean") + xlab("") + ylab(NULL)

p_box_texture <- ggplot(train, 
                        aes(y = texture_mean, x = diagnosis, fill = diagnosis)) +
  geom_boxplot(show.legend = FALSE) +
  ggtitle("texture_mean") + xlab("") + ylab(NULL)


grid.arrange(p_scatter, p_box_radius, p_box_texture, nrow = 1, widths = c(2, 1, 1))
```

## Reminder: odds and  odds-ratios

- If $p$ is the probability of having a malignant tumor, then the **odds** are 
$$
  \mathrm{Odds} = \frac{p}{1 - p}.
$$
For example: if $p = 0.9$ then $\mathrm{Odds} = 9$.

- Odds range from 0 (impossible event) to $+\infty$ (almost certain).

- **Odds ratio**: 

## Log-odds (logits)

Often it makes sense to work with the logarithm of the odds (**logits**):
$$
  \mathrm{logit} = \log \left( \frac{p}{1 - p} \right).
$$
To convert back to probabilities, use the sigmoid or **logistic** function:
$$
  p = \sigma(\mathrm{logit}) = \frac{1}{1 + e^{-\mathrm{logit}}}.
$$

Logits are unbounded: $\mathrm{logit} \to \pm\infty$ for $p \to 0, 1$

```{r}
#| fig-height: 3
ggplot(tibble(x = seq(-5, 5, length.out = 100)), aes(x)) +
  geom_function(fun = plogis) +
  xlab("Logit") +
  ylab("Probability")
```

## Regression for classification

- Notation:
  - $Y = 0, 1$: categorical outcome
  - $X_1, \ldots, X_p$: predictors (can be continuous or discrete)

- Idea: model $p(x) = P(Y = 1 | X = x)$


## Why does linear regression not work?

- One predictor $X = \mathtt{radius\_mean}$, outcome $Y = 0, 1$.
- Assume $p(x) = \alpha + \beta x$ and determine $\alpha, \beta$ through linear regression.

```{r}
#| out-width: 2in
#| out-height: 1.5in
#| fig-align: center
ggplot(train, aes(x = radius_mean, y = diagnosis_binary)) +
  geom_point(aes(color = diagnosis)) +
  stat_smooth(method="lm", se=FALSE, color = "gray40") +
  ylab("Probability")

```

Problems:

- Fitted probabilities can take on values outside $[0, 1]$.
- Does not easily generalize to more than two classes.

## Logistic regression

$Y$ is binomial with probability $p(X)$

$$
  \mathrm{logit}(x) = \alpha + \beta x
$$

## Determining the regression parameters

Determined by maximizing the **likelihood function**:
$$
  \ell(\alpha, \beta) = \prod_{i = 1}^n P(Y = y_i | X = x_i),
$$
where 
$$
  P(Y = y_i | X = x_i) = p(x_i)^{y_i}(1 - p(x_i))^{1- {y_i}}.
$$
is the probability of observing one data point $(x_i, y_i)$.

- Maximum cannot be found analytically (unlike least squares)
- Done in R using the `glm` function

## In R

\scriptsize
```{r}
#| echo: true
m_simple <- glm(diagnosis ~ radius_mean, data = train, family = "binomial")
summary(m_simple)
```

## The likelihood function

```{r}

```



## Quantifying the strength of an association

Write the logistic regression model in terms of odds as
$$
  \log \mathrm{Odds}(x) = \alpha + \beta x.
$$
After some algebra:
$$
  e^\beta = \frac{\mathrm{Odds}(x + 1)}{\mathrm{Odds(x)}}.
$$
In other words: $e^\beta$ is the odds ratio (OR) associated to a 1-unit increase in $x$.

::: {.callout-note}
## Breast cancer dataset
Here $\beta = 1.0662$, so $OR = \exp(1.0662) = 2.90$. An increase in 1 mm in tumor radius is associated with odds 
that are 2.90 times higher.
:::

## Testing the association

$H_0: \beta = 0$ and $H_A: \beta \ne 0$.

## Confidence intervals for regression parameters

## Making predictions

What is the probability of a tumor being malignant if the radius is 13 mm?

- By hand: 
  $$
    p(x = 13) = \frac{1}{1 + \exp(15.8086 - 1.0662 \times 13)} = 0.1247716
  $$
- Using R:

```{r}
#| echo: true
predict(m_simple,
        newdata = data.frame(radius_mean = 13), 
        type = "response")

```

## Making predictions

```{r}
ggplot(train, aes(x = radius_mean, y = diagnosis_binary)) +
  geom_vline(xintercept = 13, linetype = "dashed", color = "gray60") +
  geom_hline(yintercept = 0.1247961, linetype = "dashed", color = "gray60") +
  geom_jitter(aes(color = diagnosis), height = 0.10) +
  stat_smooth(method="glm", se=FALSE, color = "gray40",
              method.args = list(family=binomial))
```

## Multiple logistic regression


## In R

\scriptsize
```{r}
#| echo: true
m_multi <- glm(diagnosis ~ radius_mean + texture_mean,
               data = train, family = "binomial")
summary(m_multi)
```





# Draft outline




## Making predictions

## Multiple logistic regression

## Model fit


## More than two response classes

## Linear discriminant analysis (LDA)

Why another classification technique?

- More stable when classes are well separated
- More stable for small $n$
- Easy to use when more than two response classes

Results often surprisingly similar. No method will outperform all the others all the time.

Other classifiers:
- $k$-nearest neighbors
- Neural networks
- Support vector machines (SVM)

## Reminder: Bayes

$$
  P(Y = k | X = x) = \frac{P(X = x | Y = k) P(Y = k)}{\sum P(X = x | Y = k) P(Y = k)}
$$

Need to model:
- Prior probabilities $\pi_k$
- Class functions $f_k(x)$

We get: posterior probabilities $p_k(X)$

## Gaussian assumption

$f_k(x)$ are Gaussians with **same variance**

Derive discriminant functions


## Estimating the LDA parameters

## Worked out example

Simulated data, well separated, linear decision boundary is line orthogonal to centers (if noise is isotropic)

## In R


## More than two predictors

## Comparison logistic and linear regression


## References

- 

## Note to self

- Find compelling case studies
- Make sure decision boundary is clearly introduced
- Figure out the wald test

## Case studies

- Synthetic data
- Iris, for multiple classes
- Zuur et al (modeling ecological data): S. Solea vs salinity. 61 observations
- VGSM: WCGS dataset (large, multiple predictors)
- Le et al: Prostate cancer dataset. Multiple predictors, small-ish
- Pupal color.


